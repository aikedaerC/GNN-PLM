
## Introduction

vulsim:
Software vulnerabilities continue to be a major source of financial and reputational harm to corporations [52, 102]. Despite intensive efforts from academia and industry to mitigate software vulnerabilities, the number of reported vulnerabilities in the Common Vulnerability and Exposure (CVE) database has increased over time [62,70]. For instance, in 1999, a mere 321 CVE records were reported, whereas in 2023, this figure has skyrocketed to around 29K. Effective detection of software vulnerabilities is still a major and growing need.

One possible approach for detecting software vulnerabilities is the usage of DL models. While DL models have been used successfully in various other contexts, their adoption for vulnerability detection faces multiple major challenges.

The lack of large-scale, publicly available, and reliable labeled datasets is extensively discussed in the literature as a major challenge associated with the application of DL models in vulnerability detection [40, 114]. Note that databases such as the National Vulnerability Database (NVD) and SARD [81] do exist, but the amount of data they contain is not sufficient for training DL models. First, given the vast complexity of modern software and its large number of degrees of freedom, perhaps many millions of training samples would be necessary to train DL models, while the entirety of the NVD, spanning many decades and all types of software and vulnerability types, only contains around 220K vulnerabilities at the time of writing this paper. It is worth mentioning that, this figure represents reported vulnerabilities, requiring further processing by retrieving corresponding information from GitHub links, if available. However, the actual number ofusable vulnerable samples after processing is considerably lower, as evidenced by studies [95, 116].

Moreover, the widely used SARD [81] dataset contains synthetic samples, and as Chakrabarty et al. [13] demonstrated, real-world examples are more complex than the synthetic counterparts. The performance drop was observed to be 54% in cases where the model was trained exclusively on nonsynthesized data [13, 121]. Additionally, recent work by Chen et al. [16] demonstrates that increasing the volume of the training data does not necessarily enhance the performance of the model and can reach a saturation point. This was observed when they applied their recently complied large DiverseVul dataset to several models.

Another limitation of the current datasets is data imbalance where the number of vulnerable samples is substantially lower than non-vulnerable ones. Models trained on imbalanced datasets are biased toward non-vulnerable samples [13]. Based on this finding, in order to get a good performance, one has to include approximately an equal number of non-vulnerable samples when training DL models. This implies that we cannot utilize all the samples from the existing datasets.

To summarize, existing vulnerability datasets have a relatively small number of usable samples necessary for proper training of DL models, many existing samples are synthetic, and there is an imbalance between the vulnerable and nonvulnerable samples in the datasets. These reasons contribute to general poor performance of DL models for vulnerability detection, and even worse generalizability (e.g., F1 score in SOTA dropping from 49% to 9.4%) [13, 16, 121]. With these challenges in mind, we hypothesize and later validate the effectiveness of leveraging more properties from the existing limited data to develop a more robust vulnerability detection framework.

vul-llmgnn:
With the rapid expansion of the open-source community, software vulnerability detection technology has become a significant concern in the software industry and cybersecurity domain. Vulnerabilities pose a threat to the integrity and availability of software and computer systems, potentially leading to privilege escalation, leakage of sensitive data, denial of service, and various other attacks, resulting in substantial economic and societal losses [1]. In practice, developers and security engineers primarily rely on code analysis or testing tools to detect and repair bugs, such as rule-based analysis and symbolic execution [2]. However, these methods require extensive manual verification due to their high false-positive rates.

To improve the efficiency of code vulnerability detection, extensive research has leveraged deep learning (DL) models for automated vulnerability detection. These methods extract features from the source code to generate initial embedding vectors, which are then fed into neural networks to learn vulnerability patterns and produce classification results, thereby achieving automatic detection capabilities [3]

Deep learning-based methods for code vulnerability detection are primarily divided into two types: sequence-based and graph-based approaches. Sequence-based approaches process the source code or its structures (e.g., Abstract Syntax Trees, AST) into serialized forms and interpret individual elements as tokens, which could be entire lines of code or segments divided by spaces [4], [5]. Neural networks like RNNs, LSTMs [6], [7], GRUs, and CNNs [8] are employed for detecting and classifying vulnerabilities by extracting sequence features from the code. Although sequence-based approaches exhibit strengths in learning the contextual information of code, they fall short in effectively capturing the program's hierarchical structures, execution flows, and data and control dependencies.

Graph-based methods transform source code into heterogeneous graph structures, such as AST, Control Flow Graph (CFG), and Program Dependence Graph (PDG), to efficiently capture both local structures and dependencies within the code. These graphical representations enrich the analysis by providing intricate syntactic and semantic connections beyond mere code sequences. Leveraging code graphs, models based on GNN have demonstrated their effectiveness in extracting structural insights for vulnerability detection, as evidenced by research conducted by Wang et al. [9] and Zhou et al.[6]. Although graph-based methods provide valuable insights, they often overlook subtle coding patterns and long-distance contexts, and with their process of abstraction potentially leading to the loss of specific logic and behaviors in the code.

llm with gnn:
Graph, or graph theory, serves as a fundamental part of numerous areas in the modern world, particularly in technology, science, and logistics [Ji et al., 2021]. Graph data represents the structural characteristics between nodes, thus illuminating relationships within the graph's components. Many real-world datasets, such as citation networks [Sen et al., 2008], social networks [Hamilton et al., 2017], and molecular [Wu et al., 2018], are intrinsically represented as graphs. To tackle graph-related tasks, Graph Neural Networks (GNNs) [Kipf and Welling, 2016; Velickovic et al., 2018] have emerged as one of the most popular choices for processing and analyzing graph data. The main objective of GNNs is to acquire expressive representations at the node, edge, or graph level for different kinds of downstream tasks through recursive message passing and aggregation mechanisms among nodes.

In recent years, significant advancements have been made in Large Language Models (LLMs) like Transformers [Vaswaniet al., 2017], BERT [Kenton and Toutanova, 2019], GPT [Brown et al., 2020], and their variants. These LLMs can be easily applied to various downstream tasks with little adaptation, demonstrating remarkable performance across various natural language processing tasks, such as sentiment analysis, machine translation, and text classification [Zhao et al., 2023d]. While their primary focus has been on text sequences, there is a growing interest in enhancing the multi-modal capabilities of LLMs to enable them to handle diverse data types, including graphs [Chai et al., 2023], images [Zhang et al., 2023b], and videos [Zhang et al., 2023a].

LLMs help graph-related tasks. With the help of LLMs, there has been a notable shift in the way we interact with graphs, particularly those containing nodes associated with text attributes. As shown in Figure 1, the integration of graphs and LLMs demonstrates success in various downstream tasks across a myriad of graph domains. Integrating LLMs with traditional GNNs can be mutually beneficial and enhance graph learning. While GNNs are proficient at capturing structural information, they primarily rely on semantically constrained embeddings as node features, limiting their ability to express the full complexities of the nodes. Incorporating LLMs, GNNs can be enhanced with stronger node features that effectively capture both structural and contextual aspects. On the other hand, LLMs excel at encoding text but often struggle to capture structural information present in graph data. Combining GNNs with LLMs can leverage the robust textual understanding of LLMs while harnessing GNNs' ability to capture structural relationships, leading to more comprehensive and powerful graph learning. For example, TAPE [He et al., 2023] leverages semantic knowledge that is relevant to the nodes (i.e., papers) generated by LLMs to improve the quality of initial node embeddings in GNNs. In addition, InstructGLM [Yeet al., 2023] replaces the predictor from GNNs with LLMs, leveraging the expressive power of natural language through techniques such as flattening graphs and designing instruction prompts. MoleculeSTM [Liu et al., 2022] aligns GNNs and LLMs into the same vector space to introduce textual knowledge into graphs (i.e., molecules), thereby improving reasoning abilities.


## Related Work

### GNN-based Methods
multiple-graph:
In recent years, Graph Neural Network (GNN) [33], such as Graph Convolutional Network (GCN) [34], Graph Attention Network (GAT) [35], Gated Graph Sequence Neural Networks (GGNN) [36], has been shown to be effective in modeling graph data, e.g., social networks [12], traffic networks [13]. GNN has also been applied to software engineering tasks such as code summarization [37], clone detection [38], and bug localization [39], and has achieved impressive performance. The goal of GNN is to train a parametric function via message passing between the nodes of graphs for further tasks, i.e., graph classification, node classification, and edge prediction. 

GNN is a kind of neural network specifically designed to operate over data structured as graphs. It has gained popularity due to its ability to leverage the rich relational information in graph data. GNN accepts graph-structured data as input. A graph consists of nodes (or vertices) and edges. Each node represents an entity, and each edge represents a relationship between two entities. In addition to the graph structure, each node and edge can have associated attributes or features, represented as vectors. The input of GNN is usually a graph with some associated features for each node and edge. These features could be binary (indicating the presence or absence of a certain attribute), categorical (discrete values from a certain set), or numerical (continuous values). The graph structure is represented by an adjacency matrix that defines the connections between different nodes. The output of a GNN can be different depending on the task at hand. For node classification, the output would be a label or a set of labels for each node in the graph. For graph classification, the output would be a label or a set of labels for the entire graph. For link prediction, the output would be a set of predicted links (edges) in the graph. For graph generation, the output would be a new graph, generated based on learned patterns from the input graph.

The general architecture of a GNN involves a few key components: 1. Node Representation Learning: GNNs learn a vector representation (embedding) for each node in the graph. 2. Aggregation Function: The function used to aggregate the neighbors' representations for each node in each iteration. 3. Update Function: After aggregating the neighbors' representations, an update (or transformation) function is applied to compute the new representation for each node based on the node's current representation and its neighbors' aggregated representations. 4. Readout Function: After several iterations of node representation learning (also known as message passing), a readout function is used to aggregate all of the node representations in the graph to form a graph-level representation.

GNN differs from traditional neural networks in the following aspects: First, traditional neural networks like CNN and RNN typically deal with grid data, such as images (2D grids of pixels) or texts (1D sequences of words). In contrast, GNN is specifically designed to work with graph data, where entities (nodes) can have complex, non-grid relationships with each other. Second, in traditional neural networks, the process of feature extraction is done via predefined layers, and the connections between these layers are fixed. In GNN, the feature extraction process is dynamic and is dependent on graph structure.

For the function-level vulnerability detection task, representing a function as one or more graphs can better capture its structural information than representing it as plain text. Considering GNN is effective in encoding graph data, we also adopt GNN to capture vulnerability features from the graph representations of functions.


suvey:
The study of Zhou et al. in [13 ] can be considered as the first study in vulnerability prediction using GNNs. In this study, researchers tried to tackle the problem of vulnerability prediction by training a GNN using a custom crafted dataset. The dataset they crafted is consisted of functions written in C/C++ which extracted from 4 open-source projects Linux Kernel, QEMU, Wireshark, and FFmpeg giving in total 58965 graph samples from which the 27652 are vulnerabilities samples and the rest of them non-vulnerabilities. So, the dataset they created it's not perfectly balanced but at the same time is not extremely imbalanced as well. They proposed a DNN to treat vulnerability prediction as a binary classification problem. Their model consists of three steps, the graph embedding layer of composite code semantics, the gated graph recurrent layers and a conv layer. To evaluate their model, they used 4 ML prediction methods (Metrics + Xgboost, 3-layer BiLSTM, 3-layer BiLSTM + Att and CNN) and their model outperformed all the aforementioned methods by achieving accuracies and F1 scores around 75%. 

Wang et al. [ 5], proposed a GNN and an automated data collection mechanism for vulnerability prediction. In details, they proposed an extended version of gated graph neural network [14 ] which is a stacked neural network which consists of four embedding models based on Gated Recurrent Unit [ 15]. They feed their network with graph samples represented with an extended version of the AST, which they extend to have eight different edge types, and for the nodes, they use word2vector [ 16 ] to embed the nodes to numerical continuous vectors. Furthermore, their model aggregates information through relation graphs instead of the nodes the most GNNs propose. To train their model they used data from CVE, NVD, and open-source projects and for evaluation purposes they used samples written in C, Java, PHP, and Swift (gathered from SARD, NVD, and open-source projects). Finally, their proposed model trained as a binary and multi-class classifier achieving on average accuracy of 92%.

In [17 ] Wu et al. presented a binary classifier for vulnerability prediction in C/C++ code. Their study proposed the use of Simplified Code Property Graphs (SCPG) which combines the AST and CFG to a more descriptive graph representation of source code. Their approach can be described as three steps, the first one is the transformation of samples to graph representation using the Joern2tool, the second step is the graph representation learning and the final step is the classification. For the second step they tried three different GNNs, Graph Attention Network (GAN) [ 18], Graph Convolutional Network (GCN) [19 ], and Gated Graph Network (GGN) [ 14 ] with the first one (GAT) achieving the best performance (F1 score of 83.6%). To train their models they used the SARD dataset and to reduce its bias they applied prepossessing to each sample by replacing function names with FUN# and variables name with VAR# where # is the incremental number of each function and variable respectively. Finally, they trained two more classifiers using CNNs and RNNs in order to compare their performance with GNNs, with GNNs performing slightly better than them by achieving much better results in Recall measurement and, as such, better F1 score.

The study of Cao et al. in [ 20 ] proposed using bidirectional graph neural network (BGNN) along with CNNs to create a binary classifier for detecting vulnerabilities in code written in C/C++. To train their model they manually crafted a dataset from four opensource projects (Linux Kernel, FFmpeg, Wireshark, and Libav) and they proposed a new graph representation, the Code Composite Graph (CCG) which combines AST, CFG, and DFG to a joint directed graph with edges that link the nodes and each link has a label of AST, CFG or DFG respectively. Same as [ 17 ], they used Joern to extract graphs from the code samples and after that they vectorized the nodes using word2vec. Each node of the CCG contains the embeddings of the leaf nodes of AST concatenated with the type of the node (e.g., Identifier). Finally, their findings showed that their model outperforms the state-of-the-art (SoTA) approaches they chose to compare it with, by achieving accuracy of ∼75% and F1 score of ∼77%. Also, they showed that using directed graphs can give much better than using undirected graphs.

Nguyen et al. [ 21 ] proposed a graph neural network model for detecting vulnerabilities in multiple programming languages, so, they proposed a language agnostic mechanisms as a solution to vulnerability prediction task. They proposed a neural network consisted of a graph neural network (GCN or GGNN) with residual connections among layers followed by a readout layer for predicting vulnerabilities. In their proposed model the use of GCN achieved the best accuracy of 63.69 % which outperformed many SoTA models like Devign, CodeBERT, and GraphCodeBERT. To train and test their model they used the CodeXGLUE dataset [ 22 ] which contains in total 27518 manually labeled samples.

In study [ 23 ], Sahin et al. proposed a mechanism for predicting vulnerable versions of code using GNNs by exploiting not only code but commits as well. In particular, they tested Graph Convolutional Networks and GraphSAGE along with other deep learning and machine learning architectures. To train the GNN models they collected samples from the Wireshark project and to feed them to a GNN models they transformed the collected into AST representation. Also, they used a similar method to word2vec to learn nodes' features. To identify the location of vulnerability into the code they used the commits along with the SZZ algorithm. Finally, the use of GraphSAGE for detecting vulnerable code achieved the best score by achieving F1 score of 74.4% and AUC-ROC score of 96.0%.

In [ 24], Hin et al. presented a work on statement level vulnerability prediction. In particular, they created their own dataset of C/C++ samples to train a model that utilizes Transformers along with GNNs in a node classification problem. They used CodeBERT to obtain embeddings from functions and statements and after that they concatenate functions' embeddings obtained by codeBERT along with the graph embeddings obtained from a GAT layer to predict if a statement is vulnerable or not. Their proposed method outperformed the SoTA in vulnerability prediction in statement level, although their model achieved poor results in detecting vulnerabilities.

Similarly to [ 24 ], Cao et al. [25 ] proposed a statement-level vulnerability classifier utilizing GNNs and binary node classification for detecting memory-related vulnerabilities. In details, they trained a Flow-Sensitive GNN (FS-GNN) which embeds not only the statement nodes but flow edges as well. To feed their model with data they created their own dataset using as sources the SARD and CVE and to transform the samples into PDGs they used Joern. Furthermore, they enriched PDG representation with additional semantic information and utilized Doc2Vec [ 26 ] in order to transform statements into numerical vectors. To overcome the problem of imbalanced nodes (most of the nodes were non-vulnerable nodes) they used GraphSMOTE [ 27] which add synthetic nodes generated based on the similarity of nodes. Finally, their model (FS-GNN) achieved accuracy of 77.5 % and F1 score of 59.5 % and outperformed many static-analysis approaches (PCA, Saber, Flawfinder, Rats, and Infer) and deep learning approaches (VulDeePecker [ 7 ], SySeVR [8], and Devign [13]).

Finally, the last study presented in this paper is the one conducted by Ding et al. [28 ], which as the previous, proposes a binary node classifier for detecting vulnerability at the statement level. In this study, the researchers proposed using ensemble learning to learn both local and long-range dependencies in code. They independently trained a transformer model for capturing long-range dependencies and a GGNN for capturing local dependencies. To train their ensemble model, they first used Juliet (SARD) dataset as it contains much more samples than any publicly available dataset. After that, they fine-tuned their model using the D2A dataset, which consists of real-world code samples, in order for the model to generalize in real-world projects. For feeding with inputs, the two models (transformer and GGNN) converted samples into CPG for the GNN model, and for the transformers, they used the embeddings of the AST statements in sequence. Their approach achieved great results when evaluated in the Juliet dataset with accuracy and F1 score of∼99 %. However, evaluating it on the D2A dataset (real-world samples) did not perform that well by barely achieving an accuracy of 59.3 % and an F1 score of 59.0%. Finally, their research showed that their approach outperforms traditional static analyzers, and using an ensemble model can give better results than using a single model like GNN or transformers. Furthermore, the use of fine-tuning on a real-world dataset can mitigate the problem of generalization.

LCVD:
However, there are still limitations in the form of text sequences in terms of representation logic and structure. Therefore, recent works (Zhou et al., 2019; Cao et al., 2021; Nguyen et al., 2022; Cao et al., 2022) focus on graph-based vulnerability detection approaches. For example, Zhou et al. (2019) proposed Devign, which represents the vulnerable source code as a joint graph while using GGNN to extract vulnerability features. Cao et al. (2021) proposed BGNN4VD to extract syntactic and semantic information from source code through a variety of code graph structures and constructed a bi-directional graph neural network (BGNN) based vulnerability detection framework. Nguyen et al. (2022) proposedReGVD, which flattens the sequence of tokens into a graph structure and uses GNN with residual connections and a graph-level mixture graph pooling based on soft attention mechanisms for graph classification.


### PLM-based Methods

unveiling code pLM:
Pre-trained models have been used to support many tasks due to their excellent generalizationability in natural language processing tasks. Recently, researchers pre-trained transformers [54]using code data to solve programming tasks. According to the pre-training strategies and modelarchitectures, we can group the pre-trained models into three (3) types: auto-encoding models,auto-regressive models, and sequence-to-sequence (Seq2Seq) models. Auto-encoding modelsutilize Transformer encoders and are pre-trained with objectives such as MLM. MLM masks sometokens in the code sequence and expects the model to predict the masked tokens using bidirectionalcontext information, which in fact enables the model to use future tokens to predict current masktokens. CodeBERT [13] is pre-trained on the CodeSearchNet dataset [26]. GraphCodeBERT [17]includes one additional input type, dataflow sequence, compared with CodeBERT. CodeBERT andGraphCodeBERT use the encoder of Transformer. Auto-regressive models use CLM or its variantsto pre-train the transformers in a left-to-right manner. CodeGPT [42] uses this pre-training strat-egy and keeps the transformer decoder. Seq2Seq models, e.g., CodeT5 [71], use both an encoderand a decoder in the Transformer. CommitBART [40] uses BART [32] architecture to pre-train amodel for GitHub commits.



## Abstract


Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. 

